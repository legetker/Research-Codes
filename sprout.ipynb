{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"Sprout\" SOM Data Program\n",
    "**Authors:** Maria Molina (NCAR), modifications by Gary Lackmann (NCSU) and Lauren Getker (NCSU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "from time import time\n",
    "from sys import stdout\n",
    "import os\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "from cartopy.feature import NaturalEarthFeature\n",
    "import csv\n",
    "import datetime as dt\n",
    "import sys\n",
    "import metpy\n",
    "import metpy.calc as mpcalc\n",
    "from metpy.units import units\n",
    "from time import sleep\n",
    "import pygrib\n",
    "import glob\n",
    "\n",
    "from blossom import blossom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User Settings & Guidance\n",
    "**Variable key:** <br>\n",
    "T = temperature, U = zonal winds, V = meridional winds, H = horizontal wind speed, W = vertical winds, Z = geopotential height, Q = specific humidity, S =bulk wind difference (need to specify shear levels), R = relative vorticity <br>\n",
    "\n",
    "*Where can I read about MiniSOM?*: https://github.com/JustGlowing/minisom/blob/master/minisom.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Data user settings\n",
    "\"\"\"\n",
    "\n",
    "#Select a pressure level\n",
    "p_level = 1000\n",
    "\n",
    "#Set to 1 if you want to input a CSV of dates\n",
    "readCSV = 0\n",
    "\n",
    "#Set to 1 if you want to compute anomalies.\n",
    "comp_anomaly = 1\n",
    "\n",
    "#Select a variable U, V, W, S, H, Z, T, R, or Q.\n",
    "var = \"T\" \n",
    "\n",
    "#Name of .csv file directory with dates in yyyymmdd format. Only used if readCSV = 1\n",
    "csv_name = \"liberal_hot_days_lagged.csv\"  \n",
    "\n",
    "#The date or dates. If you want all dates in a certain position, type: \"?\". Only used if readCSV = 0\n",
    "date_pick = \"????05??\"  \n",
    "\n",
    "#Choose a time in UTC: '00:00', '06:00', '12:00' or '18:00'. Only used if readCSV = 0\n",
    "time_date_pick = '06:00'\n",
    "\n",
    "#Upper level for shear calculation. Only used if var=\"S\"\n",
    "shear_upper = 500  \n",
    "\n",
    "#Lower level for shear calculation. Only used if var=\"S\"\n",
    "shear_lower = 1000  \n",
    "\n",
    "#Westernmost longitude for subsetting. Choose -180 to 180\n",
    "#wlon = -130\n",
    "wlon = -180\n",
    "\n",
    "#Easternmost latitude for subsetting. Choose -180 to 180\n",
    "#elon = -65\n",
    "elon = -80\n",
    "\n",
    "#Southernmost latitude for subsetting. Choose -90 to 90\n",
    "slat = -15\n",
    "#lat = 30\n",
    "\n",
    "#Northernmost latitude for subsetting.Choose -90 to 90\n",
    "nlat = 15\n",
    "\n",
    "\n",
    "\n",
    "#The map projection you would like to use. You may need to change the central longitude depending on domain\n",
    "projection=ccrs.Mercator(central_longitude = 180)\n",
    "\n",
    "\"\"\"\n",
    "SOM User settings\n",
    "\"\"\"\n",
    "#SOM rows\n",
    "rows = 4\n",
    "\n",
    "#SOM columns\n",
    "cols = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Constants\n",
    "\"\"\"\n",
    "#acceleration of gravity for geopotential conversion\n",
    "g0 = 9.80665\n",
    "\n",
    "#for converting longitudes\n",
    "l0 = 360 \n",
    "\n",
    "#ERAI means path\n",
    "erai_path = \"/zephyr/erai/climo/\"\n",
    "\n",
    "\"\"\"Error messages\"\"\"\n",
    "coord_error = \"Coordinates out of bounds.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Error checking\n",
    "\"\"\"\n",
    "#Converting longitudes from (-180, 180) to (0, 360)\n",
    "if elon < 0:\n",
    "    elon = elon + 360\n",
    "if wlon < 0:\n",
    "    wlon = wlon + 360\n",
    "\n",
    "#Are coordinates within bounds?\n",
    "if (wlon > 360 or wlon < 0):\n",
    "    sys.exit(coord_error)\n",
    "if (elon > 360 or elon < 0):\n",
    "    sys.exit(coord_error)\n",
    "if (slat > 90 or slat < -90):\n",
    "    sys.exit(coord_error)\n",
    "if (nlat > 90 or nlat < -90):\n",
    "    sys.exit(coord_error)\n",
    "if (slat > nlat or wlon > elon):\n",
    "    sys.exit(coord_error)\n",
    "    \n",
    "#Does the variable exist?\n",
    "if (var != \"U\" and var != \"V\" and var != \"W\" and var != \"S\" and var != \"H\" and var != \"Z\" and var != \"T\" and var != \"Q\" and var != \"R\"):\n",
    "    sys.exit(\"Not a valid variable.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions\n",
    "This cell contains all functions might be needed later on. Function comments list the purpose of the function, the parameters (marked as \"**param**\") and return value (marked as \"**return**\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This function creates a plot with coastlines.\n",
    "param fig: the figure which should be plotted onto\n",
    "param pltLabel: the label which is added to the plot\n",
    "return ax: the created axes\n",
    "\"\"\"\n",
    "def create_plot(fig, ax, pltLabel):\n",
    "    if fig is None or ax is None:\n",
    "        sys.exit()\n",
    "    ax.add_feature(cfeature.STATES, edgecolor='black')  #Add US states\n",
    "    ax.add_feature(cfeature.COASTLINE, edgecolor='black')  #Add coastlines\n",
    "    ax.set_extent([elon,wlon,slat,nlat])  #subset to a specific region\n",
    "    gl = ax.gridlines(draw_labels=True, dms=True, x_inline=False, y_inline=False)\n",
    "    gl.top_labels=False   # suppress top labels\n",
    "    gl.right_labels=False # suppress right labels\n",
    "    ax.set_title(pltLabel)  #set title\n",
    "    return ax\n",
    "\n",
    "\"\"\"\n",
    "Gets dates and times from a csv file, adds them to lists, and returns them. Also finds the ERA Interim data file for each day, and returns a list of filenames for each case.\n",
    "Please see guide for csv file formatting specificiation. \n",
    "param csv_name: the name of the csv file with dates in yyyymmdd format and times in 24hr time\n",
    "return dates: an array containing the dates of each case\n",
    "return datafiles: an array containing the names of netCDF ERA Interim files which correspond to each date.\n",
    "returns times: an array containing the time of each case\n",
    "\"\"\"\n",
    "def get_dates_from_csv(csv_name):\n",
    "    dates = []\n",
    "    times = []\n",
    "    rownum = 0\n",
    "    with open(csv_name,'r', encoding='utf-8-sig') as f:\n",
    "        reader = csv.reader(f)\n",
    "        for row in reader:\n",
    "            [c.replace('\\ufeff', '') for c in row] #Get rid of any byte order marks--they'll cause problems later on. \n",
    "            dates.append(row[0])\n",
    "            times.append(row[1])\n",
    "            rownum += 1\n",
    "            \n",
    "    numfiles = len(dates)\n",
    "    datafiles = []\n",
    "    for i in range(numfiles):\n",
    "        datafiles.append(\"/zephyr/erai/plevel/\" + dates[i] + \".nc\")\n",
    "    \n",
    "    return dates, datafiles, times \n",
    "\n",
    "\"\"\"\n",
    "Calculates horizontal wind speed in meters/second. Throws an error if the variable was not included in the input.\n",
    "param ds: xarray object containing all the data for a given day and time.\n",
    "returns the total wind speed in m/s\n",
    "\"\"\"\n",
    "def calculate_wind_speed(dsTime):\n",
    "    try:\n",
    "        uwnd = dsTime[\"U\"].values\n",
    "        uwnd = uwnd * units.meter / units.second\n",
    "        vwnd = dsTime[\"V\"].values\n",
    "        vwnd = vwnd * units.meter / units.second\n",
    "    except:\n",
    "        sys.exit(\"Variable not found.\") \n",
    "     \n",
    "    return metpy.calc.wind_speed(uwnd, vwnd)\n",
    "\n",
    "\"\"\"\n",
    "This function rounds 24 hr times to the nearest 6 hr timestamp (0000, 0600, 1200, or 1800) and converts to datetime64. \n",
    "Throws an error if an invalid time is passed in (ie, less than 0:00 or greater than 23:59)\n",
    "param time: the time in string format with no leading zero (ie 600, not 0600)\n",
    "returns the rounded timestamp\n",
    "\"\"\"\n",
    "def format_time(time, date):\n",
    "    #Parse the time into a string\n",
    "    if int(time) >= 0 and int(time) < 1000:\n",
    "        time = time[0]\n",
    "    elif int(time) >= 1000 and int(time) < 2359:\n",
    "        time = time[0:1]  \n",
    "    else:\n",
    "        sys.exit(\"Invalid time.\")\n",
    "    \n",
    "    dateTime = pd.Timestamp(year = int(date[0:4]), month = int(date[4:6]), day = int(date[6:8]), hour = int(time))\n",
    "    dateTime = dateTime.round(freq = \"6H\")\n",
    "    pyDate = dateTime.to_pydatetime()\n",
    "    \n",
    "    return pyDate\n",
    "\n",
    "\"\"\"\n",
    "Creates a composite of a given variable for a given set of cases.\n",
    "param new_var: the variable to be averaged\n",
    "param plevel: the pressure level you would like to get data from\n",
    "returns avg: the composite data\n",
    "\"\"\"\n",
    "def get_composite(new_var, plevel):\n",
    "    dates, datafiles, times = get_dates_from_csv(csv_name)\n",
    "    numfiles = len(datafiles)\n",
    "    validTimes = []\n",
    "    for j in range(0,numfiles):\n",
    "        ncfile = datafiles[j]\n",
    "        try:\n",
    "            ds = xr.open_dataset(ncfile)\n",
    "        except:\n",
    "            print(\"File not found: \" + ncfile)\n",
    "            continue\n",
    "        validTimes.append(times[j])\n",
    "        \n",
    "        ds = xr.open_dataset(ncfile)\n",
    "        time = format_time(times[j], dates[j])\n",
    "\n",
    "        dsTime = ds.sel(level = plevel, time = time)\n",
    "        lat = ds['lat'].values\n",
    "        lon = ds['lon'].values\n",
    "        dx, dy = mpcalc.lat_lon_grid_deltas(lon, lat)\n",
    "\n",
    "        if new_var == \"H\":  #Horizontal wind speed\n",
    "            new_var_arr = calculate_wind_speed(dsTime)\n",
    "        elif new_var == \"S\":  #Shear\n",
    "            ds1 = ds.sel(level=shear_upper, time = time)\n",
    "            ds2 = ds.sel(level=shear_lower, time = time)\n",
    "            variable = calculate_wind_speed(ds1) - calculate_wind_speed(ds2)\n",
    "        elif new_var == \"R\":  #Relative Vorticity\n",
    "            uwnd = dsTime[\"U\"].values\n",
    "            uwnd = uwnd * units.meter / units.second\n",
    "            vwnd = dsTime[\"V\"].values\n",
    "            vwnd = vwnd * units.meter / units.second\n",
    "            new_var_arr = mpcalc.vorticity(uwnd, vwnd, dx = dx, dy = dy)\n",
    "        else:  #all other variables - don't require additional calculation\n",
    "            new_var_arr = dsTime[new_var].values\n",
    "\n",
    "        if new_var == 'Z':  #convert geopotential to geopoential height if necessary\n",
    "            new_var_arr = new_var_arr / g0\n",
    "        \n",
    "        #Create a DataArray with the data, then add it to one large array.    \n",
    "        lats = dsTime['lat'].values\n",
    "        lons = dsTime['lon'].values\n",
    "\n",
    "        dsnew_new = xr.DataArray(new_var_arr, coords=[lats, lons], dims=['lat', 'lon'])\n",
    "        if j == 0: \n",
    "            dscatold_new = dsnew_new\n",
    "        if j > 0:\n",
    "            dscat_new = xr.concat([dscatold_new,dsnew_new], dim='time')\n",
    "            dscatold_new = dscat_new\n",
    "            \n",
    "    #lons = lons - l0;\n",
    "    dataNew = xr.DataArray(dscatold_new, coords=[validTimes, lats, lons], dims=['time', 'lat', 'lon'])\n",
    "    dataNew = dataNew.where((dataNew['lat']<nlat) & (dataNew['lat']>slat) &   (dataNew['lon']>wlon) & (dataNew['lon']<elon) , drop=True)\n",
    "    newLats = dataNew['lat'].values\n",
    "    newLons = dataNew['lon'].values\n",
    "    \n",
    "    \n",
    "    #Average all of the cases.\n",
    "    mapnum = 0  #iterator\n",
    "    avg = np.zeros_like(SOM.comp_data, dtype = object)\n",
    "    for i in range(0,rows):\n",
    "        for j in range(0,cols):\n",
    "            cases = SOM.get_cases(i, j)\n",
    "            for k in cases:\n",
    "                avg[i,j] = avg[i,j] + dataNew[k,:,:]\n",
    "            avg[i,j]  = avg[i,j] / float(len(cases))\n",
    "            mapnum = mapnum + 1\n",
    "\n",
    "    return avg\n",
    "\n",
    "\"\"\"\n",
    "Gets the date of given a given case based on the index.\n",
    "param case: the index of the case\n",
    "\"\"\"\n",
    "def get_case_date(case):\n",
    "    return dates[case]\n",
    "\n",
    "\"\"\"\n",
    "Given the date of an event, returns the case index.\n",
    "param date: the date of the case\n",
    "return the index of the case\n",
    "\"\"\"\n",
    "def get_date_for_case(date):\n",
    "    for i in dates:\n",
    "        if dates[i] == date:\n",
    "            return i\n",
    "    sys.exit(\"Date could not be found.\")\n",
    "    \n",
    "\"\"\"\n",
    "Computes the anomaly between a given case and its associated SOM node\n",
    "param case: the index of the case in the original case array\n",
    "return anomaly: the computed anomaly\n",
    "\"\"\"\n",
    "def get_case_anomaly(case):\n",
    "    shape = SOM.find_case_node(case)\n",
    "    row = shape[0]\n",
    "    col = shape[1]\n",
    "    avgData = SOM.comp_data[row][col]\n",
    "    caseData = da[case]\n",
    "    anomaly = caseData - avgData\n",
    "    return anomaly\n",
    "\n",
    "\"\"\"\n",
    "Gets data from a grib file.\n",
    "param filename: the name of the grib file\n",
    "returns the subsetted level and variable data\n",
    "\"\"\"\n",
    "def get_mean_from_grib(filename):\n",
    "    var_names = {'PV' : 'Potential vorticity', 'Z': 'Geopotential', \\\n",
    "                'T' : 'Temperature', 'Q' : 'Specific humidity', \\\n",
    "                'W' : 'Vertical Velocity', 'V': 'V component of wind', \\\n",
    "                'U' : 'U component of wind'}\n",
    "    try:\n",
    "        grbs = pygrib.open(\"/\" + filename)\n",
    "    except:\n",
    "        sys.exit(\"Couldn't find file.\")\n",
    "    for grb in grbs:\n",
    "        if grb.name==var_names[var] and grb.typeOfLevel=='isobaricInhPa' \\\n",
    "           and grb.level==p_level:\n",
    "            return grb.values\n",
    "    sys.exit(\"Couldn't find data.\")\n",
    "    \n",
    "\n",
    "\"\"\"\n",
    "This function computes a weighted anomaly.\n",
    "param var: the variable we're getting data for\n",
    "param day_of_year: the julian date from 0 to 366\n",
    "param day_str: a string containing the date in yyyymmdd format\n",
    "return the computed anomaly for this date\n",
    "\"\"\"\n",
    "def calc_anomaly(var, day_of_year, day_str):\n",
    "    if (day_str[4:6] == \"12\"):\n",
    "        if day_of_year > 350:\n",
    "            day_diff = day_of_year - 350 \n",
    "            month_diff = 366 - 350\n",
    "            weight = day_diff / month_diff\n",
    "            if var == 'V' or var == 'U': #We need to go to a different path to get the U/V data.\n",
    "                mean_1 = get_mean_from_grib(erai_path + \"erai_means_uv\" + day_str[0:4] + \"12.grib\")\n",
    "                mean_2 = get_mean_from_grib(erai_path + \"erai_means_uv\" + day_str[0:4] + \"01.grib\")\n",
    "            else:\n",
    "                mean_1 = get_mean_from_grib(erai_path + \"erai_means/\" + day_str[0:4] + \"12.grib\")\n",
    "                mean_2 = get_mean_from_grib(erai_path + \"erai_means/\" + day_str[0:4] + \"01.grib\")\n",
    "        else:\n",
    "            day_diff = day_of_year - 319 \n",
    "            month_diff = 350 - 319\n",
    "            weight = day_diff / month_diff\n",
    "            if var == 'V' or var == 'U':\n",
    "                mean_1 = get_mean_from_grib(erai_path + \"erai_means_uv\" + day_str[0:4] + \"11.grib\")\n",
    "                mean_2 = get_mean_from_grib(erai_path + \"erai_means_uv\" + day_str[0:4] + \"12.grib\")\n",
    "            else:\n",
    "                mean_1 = get_mean_from_grib(erai_path + \"erai_means/\" + day_str[0:4] + \"11.grib\")\n",
    "                mean_2 = get_mean_from_grib(erai_path + \"erai_means/\" + day_str[0:4] + \"12.grib\")\n",
    "    elif (day_str[4:6] == \"01\" and day_of_year < 16):\n",
    "        day_diff = 16 - day_of_year\n",
    "        month_diff = 16\n",
    "        weight = day_diff / month_diff\n",
    "        if var == 'V' or var == 'U':\n",
    "            mean_1 = get_mean_from_grib(erai_path + \"erai_means_uv\" + day_str[0:4] + \"12.grib\")\n",
    "            mean_2 = get_mean_from_grib(erai_path + \"erai_means_uv\" + day_str[0:4] + \"01.grib\")  \n",
    "        else:\n",
    "            mean_1 = get_mean_from_grib(erai_path + \"erai_means/\" + day_str[0:4] + \"12.grib\")\n",
    "            mean_2 = get_mean_from_grib(erai_path + \"erai_means/\" + day_str[0:4] + \"01.grib\")  \n",
    "    else:\n",
    "        for j in range(len(day_arr)):\n",
    "            if day_of_year >= day_arr[j] and day_of_year <= day_arr[j+1]:\n",
    "                day_diff = day_of_year - day_arr[j] \n",
    "                month_diff = day_arr[j+1] - day_arr[j]\n",
    "        weight = day_diff / month_diff\n",
    "        if var == 'V' or var == 'U':\n",
    "            mean_1 = get_mean_from_grib(erai_path + \"erai_means_uv\" + day_str[0:6] + \".grib\")\n",
    "            mean_2 = get_mean_from_grib(erai_path + \"erai_means_uv\" + str(int(day_str[0:6]) + 1) + \".grib\")\n",
    "        else:\n",
    "            mean_1 = get_mean_from_grib(erai_path + \"erai_means/\" + day_str[0:6] + \".grib\")\n",
    "            mean_2 = get_mean_from_grib(erai_path + \"erai_means/\" + str(int(day_str[0:6]) + 1) + \".grib\")\n",
    "        \n",
    "    mean = weight * mean_1 + (1 - weight) * mean_2\n",
    "    if var == 'Z':\n",
    "        mean = mean / 9.8\n",
    "    anomaly = variable - mean\n",
    "    return anomaly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading and handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This cell handles getting the ERA Interim data for each date.\n",
    "\"\"\"\n",
    "if (not readCSV): #Get dates manually\n",
    "    datafiles = glob.glob(\"/zephyr/erai/plevel/\" + date_pick + \".nc\")\n",
    "    if (len(datafiles) == 0):\n",
    "        sys.exit(\"Hmmm, it looks like no data was found.\")\n",
    "    numfiles=len(datafiles)\n",
    "    timesPd = pd.date_range('2000-01-01T' + time_date_pick + ':00', periods=numfiles)\n",
    "    times = timesPd.time\n",
    "else: #Get dates from a .csv file\n",
    "    dates, datafiles, times = get_dates_from_csv(csv_name)\n",
    "    if (len(datafiles) == 0):\n",
    "        sys.exit(\"Hmmm, it looks like no data was found.\")\n",
    "    numfiles = len(datafiles)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We need to iterate through each file, get the data, and then place the data into an array.\n",
    "If the file or data could not be found for some reason, a message is printed with the name of the file, and the program continues.\n",
    "\"\"\"\n",
    "validTimes = []\n",
    "for i in range(0,numfiles): \n",
    "    ncfile = datafiles[i]\n",
    "    try:\n",
    "        ds = xr.open_dataset(ncfile)\n",
    "    except:\n",
    "        print(\"File not found: \" + ncfile)\n",
    "        continue\n",
    "        \n",
    "    validTimes.append(times[i]) #If the file is found, add that case to a list.\n",
    "    \n",
    "    if (readCSV):\n",
    "        time = format_time(times[i], dates[i])\n",
    "        day_of_year = time.timetuple().tm_yday\n",
    "    else:\n",
    "        time = times[i]\n",
    "        day_of_year = timesPd[i].dayofyear\n",
    "    \n",
    "    dsTimeAndLevel = ds.sel(level=p_level, time = time) #Subset the data to a specified time and level.\n",
    "    if (not readCSV):\n",
    "        dsTimeAndLevel = dsTimeAndLevel.drop('time')\n",
    "        \n",
    "    lat = dsTimeAndLevel['lat'].values\n",
    "    lon = dsTimeAndLevel['lon'].values\n",
    "    dx, dy = mpcalc.lat_lon_grid_deltas(lon, lat) #used in relative vorticity calculation\n",
    "    \n",
    "    if var == \"H\":  #Horizontal wind speed\n",
    "        variable = calculate_wind_speed(dsTimeAndLevel)\n",
    "    elif var == \"S\":  #Shear\n",
    "        ds1 = ds.sel(level=shear_upper, time = time)\n",
    "        ds2 = ds.sel(level=shear_lower, time = time)\n",
    "        variable = calculate_wind_speed(ds1) - calculate_wind_speed(ds2)\n",
    "    elif var == \"R\":  #Relative Vorticity\n",
    "        uwnd = dsTimeAndLevel[\"U\"].values\n",
    "        vwnd = dsTimeAndLevel[\"V\"].values\n",
    "        variable = mpcalc.vorticity(uwnd, vwnd, dx = dx, dy = dy)\n",
    "    else:  #all other variables - don't require additional calculation\n",
    "        variable = dsTimeAndLevel[var].values\n",
    "        \n",
    "    if var == 'Z':  #convert geopotential to geopoential height if necessary\n",
    "        variable = variable / g0\n",
    "         \n",
    "    \"\"\"\n",
    "    This section computes anomalies if specified by the user. We will weight the anomaly based on day of the month--for instance, \n",
    "    for the date October 10th, we would get the data for that date and subtract away the averaged means for October and September.\n",
    "    \"\"\"\n",
    "    if (comp_anomaly):\n",
    "        day_arr = [0, 16, 45, 75, 105, 136, 166, 197, 228, 258, 289, 319, 350, 366]\n",
    "        day_str = datafiles[i][20:28]\n",
    "        day_of_year = time.timetuple().tm_yday\n",
    "        month = day_str[4:6]\n",
    "        variable = calc_anomaly(var, day_of_year, day_str)\n",
    "        \n",
    "    #Create an DataArray with the data, then add it to one large array.    \n",
    "    lats = dsTimeAndLevel['lat'].values\n",
    "    lons = dsTimeAndLevel['lon'].values\n",
    "    dsnew = xr.DataArray(variable, coords=[lats, lons], dims=['lat', 'lon'])\n",
    "    if i == 0: \n",
    "        dscatold = dsnew\n",
    "    if i > 0:\n",
    "        dscat = xr.concat([dscatold,dsnew], dim='time')\n",
    "        dscatold = dscat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ds['time'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Now, format the array to be passed into MiniSOM\n",
    "\"\"\"\n",
    "#Create an xarray with the data over a specified time.\n",
    "lats = dscatold['lat'].values\n",
    "lons = dscatold['lon'].values\n",
    "da = xr.DataArray(dscatold, coords=[validTimes, lats, lons], dims=['time', 'lat', 'lon'])\n",
    "da.attrs['standard_name'] = str(p_level) + \" \" + var\n",
    "\n",
    "#Subset the data.\n",
    "da = da.where((da['lat']<nlat) & (da['lat']>slat) &  (da['lon']>wlon) & (da['lon']<elon), drop = True)\n",
    "\n",
    "#redefine lats and lons after subsetting.\n",
    "lats = da['lat'].values\n",
    "lons = da['lon'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get one time from the data and plot it to make sure it looks correct.\n",
    "print(da.shape)\n",
    "fig = plt.figure(figsize=(16,16))\n",
    "ax = plt.axes(projection=projection)\n",
    "ax = create_plot(fig, ax, da.attrs['standard_name'])\n",
    "ax.set_extent([wlon, elon, slat, nlat], crs=ccrs.PlateCarree())\n",
    "cs = ax.contourf(lons, lats, da[1,:, :], cmap = \"jet\",transform=ccrs.PlateCarree(), levels=20)\n",
    "cax = fig.add_axes([ax.get_position().x1+0.05,ax.get_position().y0,0.02,ax.get_position().height])  #You can change the numbers to move the colorbar.\n",
    "plt.colorbar(cs, cax = cax).set_label(var, size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot a histogram to see the distribution of variables.\n",
    "da=xr.DataArray(da, coords=[validTimes, lats, lons], dims=['time', 'lat', 'lon'])\n",
    "da.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SOM Training and analysis using blosSOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Train the SOM.\n",
    "\"\"\"\n",
    "SOM = blossom(da, rows, cols)\n",
    "SOM.make_SOM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "som_data = SOM.comp_data\n",
    "#add_data = get_composite('Z', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now, we can get the composite data for each SOM node and plot it over a world map. Credit: TC\n",
    "fig, axs = plt.subplots(SOM.rows, SOM.cols, subplot_kw={'projection': projection}, figsize=(24,12))  #Fig size may need to be changed to look nice.\n",
    "mapnum = 0  #iterator\n",
    "#For each cell, plot coastlines then contour the data.\n",
    "for x in range(0,SOM.rows):\n",
    "    for y in range(0,SOM.cols):\n",
    "        #get data:\n",
    "        data = som_data[x][y]\n",
    "        #add_plot_data = add_data[x][y]\n",
    "        #Plotting stuff\n",
    "        axs[x,y].add_feature(cfeature.STATES, edgecolor='black')  #Add US states\n",
    "        axs[x,y].add_feature(cfeature.COASTLINE, edgecolor='black')  #Add coastlines\n",
    "        axs[x,y].set_extent([elon,wlon,slat,nlat],crs=ccrs.PlateCarree())  #subset to a specific region\n",
    "        gl = axs[x,y].gridlines(draw_labels=True, dms=True, x_inline=False, y_inline=False)\n",
    "        gl.top_labels=False   # suppress top grid labels\n",
    "        gl.right_labels=False # suppress right grid labels\n",
    "        axs[x,y].set_title(f\"Cases: {len(SOM.get_cases(x,y))}\")  #set title\n",
    "        axs[x,y].plot([360 - 100.951480], [37.042110],  marker='o', transform=ccrs.PlateCarree())\n",
    "        #Contour the data\n",
    "        #cs1 = axs[x,y].contour(lons, lats, add_plot_data, colors = 'black', transform=ccrs.PlateCarree(), levels = 10) \n",
    "        cs2 = axs[x,y].contourf(lons, lats, data, cmap = 'bwr', transform=ccrs.PlateCarree(), levels = 20)#, levels=np.arange(-.004, .004, .0001), extend = 'both')#50, vmax = 100, vmin = -100)\n",
    "        #axs[x,y].clabel(cs1)\n",
    "        #Iterate to the next map\n",
    "        mapnum = mapnum + 1\n",
    "\n",
    "#Add a colorbar\n",
    "fig.suptitle(str(p_level) + \" \" + var, fontsize = 24)\n",
    "cbar = plt.colorbar(cs2,ax=fig.get_axes(), pad=0.04)\n",
    "cbar.set_label(str(p_level) + \" \" + var, fontsize = 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MEA443",
   "language": "python",
   "name": "mea443"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
